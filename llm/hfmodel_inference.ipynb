{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noedurandard/miniconda3/envs/env_langchain/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from questionnaire import LikertQuestionnaire, Questionnaire\n",
    "from questionnaire import AdministerCustom\n",
    "from questionnaire.utils.pct_viz import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"cerebras/Cerebras-GPT-111M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_method(\n",
    "    prompt,\n",
    "    **kwargs\n",
    "):\n",
    "    tokenized_prompt = tokenizer.encode(\n",
    "        prompt,\n",
    "        return_tensors='pt', \n",
    "    )\n",
    "    id_last_token = tokenized_prompt.shape[1]-1\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            tokenized_prompt,\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "    #return output, id_last_token\n",
    "    return output[\"scores\"][id_last_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_ids(\n",
    "    tokenizer,\n",
    "    inputs:list,\n",
    "    prefixes = [],\n",
    "    suffixes = [],\n",
    "    token_id = -1,\n",
    "    check_decode:bool=False # TODO\n",
    "):\n",
    "    tokens_ids_dict = {\n",
    "        input: list(set(\n",
    "            [tokenizer.encode(input)[token_id]]+[ # keep first token\n",
    "                tokenizer.encode(pre+input)[token_id] # !!! quite sketchy\n",
    "                for pre in prefixes\n",
    "            ]+[\n",
    "                tokenizer.encode(input+suf)[token_id]\n",
    "                for suf in suffixes\n",
    "            ]\n",
    "        ))\n",
    "        for input in inputs\n",
    "    }\n",
    "\n",
    "    if check_decode:\n",
    "        for k, values in tokens_ids_dict.items():\n",
    "            tokens_ids_dict[k] = [\n",
    "                v for v in values\n",
    "                if k in tokenizer.decode(v)\n",
    "            ]\n",
    "    \n",
    "    return tokens_ids_dict\n",
    "\n",
    "def get_tokens_prob(\n",
    "    logits,\n",
    "    token_ids:list,\n",
    "    normalize:bool=True,\n",
    "):\n",
    "    soft_m = torch.nn.functional.softmax(logits).to('cpu')[0]\n",
    "    probs = {\n",
    "        k: np.sum([soft_m[id] for id in ids])\n",
    "        for k, ids in token_ids.items()\n",
    "    }\n",
    "    \n",
    "    if normalize:\n",
    "        tot = np.sum(list(probs.values()))\n",
    "        for k in probs.keys():\n",
    "            probs[k] = probs[k]/tot\n",
    "    \n",
    "    return probs\n",
    "\n",
    "def output_parser(\n",
    "    logits,\n",
    "    choices_keys,\n",
    "):\n",
    "    choice_ids = get_tokens_ids(\n",
    "        tokenizer,\n",
    "        choices_keys,\n",
    "        prefixes = [], # TODO ?\n",
    "        suffixes = [], # TODO ?\n",
    "        check_decode = False,\n",
    "    )\n",
    "    print(choice_ids)\n",
    "    probs = get_tokens_prob(\n",
    "        logits, choice_ids, normalize = True,\n",
    "    )\n",
    "    # if hard_scores:\n",
    "    #     probs = [\n",
    "    #         {\n",
    "    #             k:int(v==max(question_p.values()))\n",
    "    #             for k,v in question_p.items()\n",
    "    #         }\n",
    "    #         for question_p in probs\n",
    "    #     ]\n",
    "    return probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noedurandard/miniconda3/envs/env_langchain/lib/python3.11/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8141, -2.4483, -4.2667,  ..., -6.1063, -4.9876, -2.3713]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_method(\"This is a test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': [32], 'B': [33], 'C': [34]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k3/w73xhqj10c7_l7qt835scd4c0000gn/T/ipykernel_93614/3360443516.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  soft_m = torch.nn.functional.softmax(logits).to('cpu')[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'A': 0.6845887, 'B': 0.29187033, 'C': 0.023540985}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser(generate_method(\"The first letter of the alphabet is: \"), [\"A\", \"B\", \"C\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_questionnaire = LikertQuestionnaire.from_json(\n",
    "    \"../data/pct.json\",\n",
    "    **{\n",
    "        \"prompt_template\":\"You can only choose one option. Respond only with the label of your answer. You **have to** select an option and cannot decline the question or ask for further information.\\n{question}\\n{choices}\\nYour choice:\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl_q, smpl_c, smpl_s = pct_questionnaire[:2]\n",
    "\n",
    "smpl_questionnaire = Questionnaire(\n",
    "    categories=pct_questionnaire.categories,\n",
    "    questions=smpl_q,\n",
    "    choices=smpl_c,\n",
    "    scores=smpl_s,\n",
    "    \n",
    "    index_typetr=\"numerical\",\n",
    "    choice_delim=\") \",\n",
    "    prompt_template=\"You can only choose one option. Respond only with the label of your answer. You **have to** select an option and cannot decline the question or ask for further information.\\n{question}\\n{choices}\\nYour choice:\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(smpl_questionnaire.make_prompts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "administer_model = AdministerCustom(\n",
    "    questionnaire=smpl_questionnaire,\n",
    "    generation_method=generate_method,\n",
    "    output_parser=output_parser,\n",
    "    generation_args={'max_new_tokens':128},\n",
    "    parser_args={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': [32], 'B': [33], 'C': [34], 'D': [35]}\n",
      "{'A': [32], 'B': [33], 'C': [34], 'D': [35]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k3/w73xhqj10c7_l7qt835scd4c0000gn/T/ipykernel_93614/3360443516.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  soft_m = torch.nn.functional.softmax(logits).to('cpu')[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'economic': 0.3682043869048357, 'social': -0.011553947742168724}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "administer_model.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'A': 0.22224994, 'B': 0.3890147, 'C': 0.11114146, 'D': 0.27759397},\n",
       " {'A': 0.14385116, 'B': 0.08616197, 'C': 0.04874725, 'D': 0.72123957}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "administer_model.answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
