{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b>\n",
    "This notebook provides a basic walkthrough the administering of a questionnaire to HF model either using a <a href=\"#locally\">model loaded locally</a>, or <a href=\"#api\">through the API</a>.\n",
    "\n",
    "This notebook is made for developping and showcasing endeavors. Yet, the model used here (locally) is most likely not powerful enough to provide meaningful answers to the prompts.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from llm.administer_llm import *\n",
    "from questionnaire import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Questionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals_qa = TFQuestionnaire.from_json(\n",
    "    \"../data/assert_capital_cities.json\",\n",
    "    **{\n",
    "        \"prompt_template\": \"Provide only the index of the correct answer.\\n{question}\\n{choices}\\nYour answer:\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provide only the index of the correct answer.\n",
      "The capital city of Belgium is Paris.\n",
      "A. True\n",
      "B. False\n",
      "Your answer:\n"
     ]
    }
   ],
   "source": [
    "print(capitals_qa.make_prompts()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# locally hosted LLM <a class=\"anchor\" id=\"locally\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_hf = AdministerHF(\n",
    "    questionnaire=capitals_qa,\n",
    "    model_id=\"cerebras/Cerebras-GPT-111M\",\n",
    "    logits_based = True,\n",
    "    local=True,\n",
    "    generation_args={\n",
    "        \"max_new_tokens\":128\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what's going on under the hood?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9662,  2.0158,  1.5350,  ..., -7.1803, -6.4043,  7.5340]])\n"
     ]
    }
   ],
   "source": [
    "generated_response = lab_hf.generation_method(\n",
    "    capitals_qa.make_prompts()[0]\n",
    ") # outputs logits \n",
    "print(generated_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': [32], 'B': [33]}\n",
      "{'A': np.float32(0.3759568), 'B': np.float32(0.62404317)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noedurandard/Desktop/question_llm/examples/../llm/utils.py:51: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  soft_m = torch.nn.functional.softmax(logits).to('cpu')[0]\n"
     ]
    }
   ],
   "source": [
    "probs = lab_hf.output_parser(\n",
    "    generated_response,\n",
    "    capitals_qa.get_choices_keys()[0],\n",
    ") # retrieves compute on \"choice\" tokens and normalize\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `run`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': [32], 'B': [33]}\n",
      "{'A': [32], 'B': [33]}\n",
      "{'A': [32], 'B': [33]}\n",
      "{'A': [32], 'B': [33]}\n",
      "{'A': [32], 'B': [33]}\n",
      "{'Correct': np.float32(1.9925859), 'Incorrect': np.float32(3.007414)}\n"
     ]
    }
   ],
   "source": [
    "results = lab_hf.run()\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HF API <a class=\"anchor\" id=\"api\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "lab_hf = AdministerHF(\n",
    "    questionnaire=capitals_qa,\n",
    "    model_id=model_id,\n",
    "    logits_based = False,\n",
    "    local=False,\n",
    "    store_answers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noedurandard/miniconda3/envs/env_dhai/lib/python3.11/site-packages/huggingface_hub/inference/_generated/types/base.py:139: FutureWarning: Accessing 'ChatCompletionOutput' values through dict is deprecated and will be removed from version '0.25'. Use dataclass attributes instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Correct': 5, 'Incorrect': 0}\n"
     ]
    }
   ],
   "source": [
    "results = lab_hf.run()\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'A': 0, 'B': 1},\n",
       " {'A': 1, 'B': 0},\n",
       " {'A': 1, 'B': 0},\n",
       " {'A': 0, 'B': 1},\n",
       " {'A': 0, 'B': 1}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_hf.answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>\n",
      "Provide only the index of the correct answer.\n",
      "The capital city of Belgium is Paris.\n",
      "A. True\n",
      "B. False\n",
      "Your answer:\n",
      "B\n",
      ">>>>>>>>>>>>>>\n",
      "Provide only the index of the correct answer.\n",
      "The capital city of Angola is Luanda.\n",
      "A. True\n",
      "B. False\n",
      "Your answer:\n",
      "A\n",
      "\n",
      "(Note: A is the index of the correct answer, which is \"True\".)\n",
      ">>>>>>>>>>>>>>\n",
      "Provide only the index of the correct answer.\n",
      "The capital city of New-Zealand is Wellington.\n",
      "A. True\n",
      "B. False\n",
      "Your answer:\n",
      "A.\n",
      ">>>>>>>>>>>>>>\n",
      "Provide only the index of the correct answer.\n",
      "The capital city of India is Katmandou.\n",
      "A. True\n",
      "B. False\n",
      "Your answer:\n",
      "B.\n",
      ">>>>>>>>>>>>>>\n",
      "Provide only the index of the correct answer.\n",
      "The capital city of Canada is Toronto.\n",
      "A. True\n",
      "B. False\n",
      "Your answer:\n",
      "B\n"
     ]
    }
   ],
   "source": [
    "for p, a in zip(capitals_qa.make_prompts(), lab_hf.generated_responses):\n",
    "    print(\">>>>>>>>>>>>>>\")\n",
    "    print(p)\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dhai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
